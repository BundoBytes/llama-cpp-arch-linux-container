services:
  llama-cpp:
    container_name: llama-cpp-nvidia-container
    build:
      context: .
      dockerfile: Dockerfile
    image: llama-cpp-nvidia-image

    volumes:
      - ..:/home/dev/projects/
      - ./models:/home/dev/llama.cpp/build/bin/models
    ports:
      - '8000:8000'
    stdin_open: true
    tty: true
    devices:
      - nvidia.com/gpu=all
